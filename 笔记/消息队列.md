消息队列已经逐渐成为企业IT系统内部通信的核心手段。它具有低耦合、可靠投递、广播、流量控制、最终一致性等一系列功能，成为异步RPC的主要手段之一。
# 1 消息队列实现
[消息队列设计精要](https://tech.meituan.com/2016/07/01/mq-design.html)

![[消息队列-消息队列实现.png]]
一般来讲，设计消息队列的整体思路是先build一个整体的数据流,例如producer发送给broker,broker发送给consumer,consumer回复消费确认，broker删除/备份消息等。 利用RPC将数据流串起来。然后考虑RPC的高可用性，尽量做到无状态，方便水平扩展。 之后考虑如何承载消息堆积，然后在合适的时机投递消息，而处理堆积的最佳方式，就是存储，存储的选型需要综合考虑性能/可靠性和开发维护成本等诸多因素。 为了实现广播功能，我们必须要维护消费关系，可以利用zk/config server等保存消费关系。 在完成了上述几个功能后，消息队列基本就实现了。然后我们可以考虑一些高级特性，如可靠投递，事务特性，性能优化等。 下面我们会以设计消息队列时重点考虑的模块为主线，穿插灌输一些消息队列的特性实现方法，来具体分析设计实现一个消息队列时的方方面面。
## 1.1 队列基本功能
### 1.1.1 RPC通信协议

所谓消息队列，无外乎两次RPC加一次转储，当然需要消费端最终做消费确认的情况是三次RPC。既然是RPC，就必然牵扯出一系列话题，什么负载均衡啊、服务发现啊、通信协议啊、序列化协议等
这一块可以选择现有的RPC框架，也可以仿造kafka等自定协议
### 1.1.2 高可用
所有的高可用都依赖于RPC和存储的高可用。一般的RPC框架自身都具有服务自动发现，负载均衡等功能，而消息队列只需要额外保证broker接受消息和确认消息的接口是幂等的，并且consumer处理消息是幂等的
### 1.1.3 服务端承载消息堆积的能力
为了满足我们错峰/流控/最终可达等一系列需求，需要把消息存储下来，然后选择时机投递。
这个存储可以是多种方式的，总的来说可以分持久化与非持久化两种
持久化的形式能更大程度地保证消息的可靠性（如断电等不可抗外力），并且理论上能承载更大限度的消息堆积（外存的空间远大于内存）。
非持久化形式的投递性能与吞吐量都会更好
### 1.1.4 存储子系统的选择
理论上，从速度来看，文件系统>分布式KV（持久化）>分布式文件系统>数据库，而可靠性却截然相反。
### 1.1.5 消费关系解析
在初步具备了转储消息的能力后，我们需要能正确的进行消息投递。
消息投递基本上是两个类别：单播与广播；单播就是点到点；多播就是一点对多点。对于绝大多数应用来说，组间广播，组内单播是最常见的情形
一般比较通用的设计是支持组间广播，不同的组注册不同的订阅。组内的不同机器，如果注册一个相同的ID，则单播；如果注册不同的ID(如IP地址+端口)，则广播。 至于广播关系的维护，一般由于消息队列本身都是集群，所以都维护在公共存储上，如config server、zookeeper等。维护广播关系所要做的事情基本是一致的：1. 发送关系的维护。2. 发送关系变更时的通知。

## 1.2 队列高级特性设计

### 1.2.1 可靠投递（最终一致性）
想要做到完全不丢消息，完全是可能的，前提是消息可能会重复，并且，在异常情况下，要接受消息的延迟。 方案说简单也简单，就是每当要发生不可靠的事情（RPC等）之前，先将消息落地，然后发送。当失败或者不知道成功失败（比如超时）时，消息状态是待发送，定时任务不停轮询所有待发送消息，最终一定可以送达。
对于各种不确定（超时、down机、消息没有送达、送达后数据没落地、数据落地了回复没收到），其实对于发送方来说，都是一件事情，就是消息没有送达。 重推消息所面临的问题就是消息重复。重复和丢失就像两个噩梦，你必须要面对一个。
#### 1.2.1.1 消费确认
当broker把消息投递给消费者后，消费者可以立即响应我收到了这个消息。但收到了这个消息只是第一步，我能不能处理这个消息却不一定。或许因为消费能力的问题，系统的负荷已经不能处理这个消息；或者是刚才状态机里面提到的消息不是我想要接收的消息，主动要求重发。 把消息的送达和消息的处理分开，这样才真正的实现了消息队列的本质-解耦。所以，允许消费者主动进行消费确认是必要的。
#### 1.2.1.2 重复消息和顺序消息
一个主流消息队列的设计范式里，应该是不丢消息的前提下，尽量减少重复消息，不保证消息的投递顺序。
谈到重复消息，主要是两个话题：

1. 如何鉴别消息重复，并幂等的处理重复消息。
2. 一个消息队列如何尽量减少重复消息的投递。
鉴别消息重复需要每一个消息应有一个唯一身份，不论是也无妨自定义还是系统生成的MessageID
处理方式有两个，一是版本号，二是状态机；
#### 1.2.1.3 中间件对于重复消息的处理
1. broker记录MessageId，直到投递成功后清除，重复的ID到来不做处理，这样只要发送者在清除周期内能够感知到消息投递成功，就基本不会在server端产生重复消息。
2. 对于server投递到consumer的消息，由于不确定对端是在处理过程中还是消息发送丢失的情况下，有必要记录下投递的IP地址。决定重发之前询问这个IP，消息处理成功了吗？如果询问无果，再重发。
### 1.2.2 事务
解决方案从大方向上有两种：
1. 两阶段提交，分布式事务。
2. 本地事务，本地落地，补偿发送。
### 1.2.3 push还是pull
#### 1.2.3.1 慢消费
慢消费无疑是push模型最大的致命伤
如果消费者的速度比发送者的速度慢很多，势必造成消息在broker的堆积。假设这些消息都是有用的无法丢弃的，消息就要一直在broker端保存。当然这还不是最致命的，最致命的是broker给consumer推送一堆consumer无法处理的消息，consumer不是reject就是error，然后来回踢皮球。
反观pull模式，consumer可以按需消费，不用担心自己处理不了的消息来骚扰自己，而broker堆积消息也会相对简单，无需记录每一个要发送消息的状态，只需要维护所有消息的队列和偏移量就可以了。
#### 1.2.3.2 消息延迟与忙等
这是pull模式最大的短板。
由于主动权在消费方，消费方无法准确地决定何时去拉取最新的消息。如果一次pull取到消息了还可以继续去pull，如果没有pull取到则需要等待一段时间重新pull。 但等待多久就很难判定了。
#### 1.2.3.3 顺序消息
如果push模式的消息队列，支持分区，单分区只支持一个消费者消费，并且消费者只有确认一个消息消费后才能push送另外一个消息，还要发送者保证全局顺序唯一，听起来也能做顺序消息，但成本太高了，尤其是必须每个消息消费确认后才能发下一条消息，这对于本身堆积能力和慢消费就是瓶颈的push模式的消息队列，简直是一场灾难。
pull模式，如果想做到全局顺序消息，就相对容易很多：

1. producer对应partition，并且单线程。
2. consumer对应partition，消费确认（或批量确认），继续消费即可。
# 2 Kafka的实现
[看完这篇Kafka，你也许就会了Kafka](https://cloud.tencent.com/developer/article/2081270)
[深入理解kafka的架构与工作原理](https://blog.csdn.net/yu1415487509/article/details/136742531)

![[消息队列-kafka实现-架构图.png.png]]
## 2.1 基本概念
kafka名词解释和工作方式：

| 名词                  | 解释                                                                                                               |
| ------------------- | ---------------------------------------------------------------------------------------------------------------- |
| Producer            | 消息生产者，就是向kafka broker发消息的客户端。                                                                                    |
| Consumer            | 消息消费者，向kafka broker取消息的客户端                                                                                       |
| Consumer Group （CG） | 个消费者组由一个或多个消费者实例组成，它们共同消费同一个Topic的分区。                                                                            |
| Broker              | 消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。                                                             |
| Topic               | 一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。                                               |
| Partition           | topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。                                                          |
| Segment             | partition物理上由多个segment组成                                                                                         |
| offset              | 每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.        |
| Replica             | 副本Replication，为保证集群中某个节点发生故障，节点上的Partition数据不丢失，Kafka可以正常的工作，Kafka提供了副本机制，一个Topic的每个分区有若干个副本，一个Leader和多个Follower |
| Leader              | 每个分区多个副本的主角色，生产者发送数据的对象，以及消费者消费数据的对象都是Leader。                                                                    |
| Follower            | 每个分区多个副本的从角色，实时的从Leader中同步数据，保持和Leader数据的同步，Leader发生故障的时候，某个Follower会成为新的Leader。                                 |
## 2.2 存储结构
![[消息队列-kafka-存储结构.png]]
kafka的存储结构基本如上图
### 2.2.1 逻辑层
#### 2.2.1.1 Topic+Partition 的两层结构
kafka 对消息进行了两个层级的分类，分别是 topic 主题和 partition 分区。

将一个主题划分成多个分区的好处是显而易见的。多个分区可以为 kafka 提供可伸缩性、水平扩展的能力，同时对分区进行冗余还可以提高数据可靠性。

不同的分区还可以部署在不同的 broker 上，加上冗余副本就提高了可靠性。
#### 2.2.1.2 Offset
对于追加日志格式，新来的数据只需要往文件末尾追加即可。
对于有多个分区的主题来说，每一个消息都有对应需要追加到的分区（分区器），这个消息在所在的分区中都有一个唯一标识，就是 offset 偏移量
这样的结构具有如下的特点：
- 分区提高了写性能，和数据可靠性；  
- 消息在分区内保证顺序性，但跨分区不保证。  
![[消息队列-kafka-存储结构-offset.png]]
### 2.2.2 物理层
#### 2.2.2.1 日志文件
kafka 使用日志追加的方式来存储数据，新来的数据只要往日志文件的末尾追加即可，这样的方式提高了写的性能。segment文件生命周期由服务端配置参数决定，默认为1GB，存活时长为7天
#### 2.2.2.2 日志索引
kafka 维护了两种索引：偏移量索引和时间戳索引
##### 2.2.2.2.1 偏移量索引
kafka 维护的是一个稀疏索引（sparse index），即不是所有的消息都有一个对应的位置，对于没有位置映射的消息来说，一个二分查找就可以解决了。
##### 2.2.2.2.2 时间戳索引
时间戳索引是一个二级索引，现根据时间戳找到偏移量，然后就可以使用偏移量索引找到对应的消息位置了。
### 2.2.3 零拷贝
为了进一步提升性能，kafka 使用了sendfile零拷贝的技术。

零拷贝简单来说就是在内核态直接将文件内容复制到网卡设备上，减少了内核态与用户态之间的切换。
[零拷贝技术](https://blog.csdn.net/m0_62645012/article/details/139268955)


如何写入数据
如何读取数据
注册中心
RPC
事务
可靠投递