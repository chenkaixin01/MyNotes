消息队列已经逐渐成为企业IT系统内部通信的核心手段。它具有低耦合、可靠投递、广播、流量控制、最终一致性等一系列功能，成为异步RPC的主要手段之一。
# 1 消息队列实现
[消息队列设计精要](https://tech.meituan.com/2016/07/01/mq-design.html)

![[消息队列-消息队列实现.png]]
一般来讲，设计消息队列的整体思路是先build一个整体的数据流,例如producer发送给broker,broker发送给consumer,consumer回复消费确认，broker删除/备份消息等。 利用RPC将数据流串起来。然后考虑RPC的高可用性，尽量做到无状态，方便水平扩展。 之后考虑如何承载消息堆积，然后在合适的时机投递消息，而处理堆积的最佳方式，就是存储，存储的选型需要综合考虑性能/可靠性和开发维护成本等诸多因素。 为了实现广播功能，我们必须要维护消费关系，可以利用zk/config server等保存消费关系。 在完成了上述几个功能后，消息队列基本就实现了。然后我们可以考虑一些高级特性，如可靠投递，事务特性，性能优化等。 下面我们会以设计消息队列时重点考虑的模块为主线，穿插灌输一些消息队列的特性实现方法，来具体分析设计实现一个消息队列时的方方面面。
## 1.1 队列基本功能
### 1.1.1 RPC通信协议

所谓消息队列，无外乎两次RPC加一次转储，当然需要消费端最终做消费确认的情况是三次RPC。既然是RPC，就必然牵扯出一系列话题，什么负载均衡啊、服务发现啊、通信协议啊、序列化协议等
这一块可以选择现有的RPC框架，也可以仿造kafka等自定协议
### 1.1.2 高可用
所有的高可用都依赖于RPC和存储的高可用。一般的RPC框架自身都具有服务自动发现，负载均衡等功能，而消息队列只需要额外保证broker接受消息和确认消息的接口是幂等的，并且consumer处理消息是幂等的
### 1.1.3 服务端承载消息堆积的能力
为了满足我们错峰/流控/最终可达等一系列需求，需要把消息存储下来，然后选择时机投递。
这个存储可以是多种方式的，总的来说可以分持久化与非持久化两种
持久化的形式能更大程度地保证消息的可靠性（如断电等不可抗外力），并且理论上能承载更大限度的消息堆积（外存的空间远大于内存）。
非持久化形式的投递性能与吞吐量都会更好
### 1.1.4 存储子系统的选择
理论上，从速度来看，文件系统>分布式KV（持久化）>分布式文件系统>数据库，而可靠性却截然相反。
### 1.1.5 消费关系解析
在初步具备了转储消息的能力后，我们需要能正确的进行消息投递。
消息投递基本上是两个类别：单播与广播；单播就是点到点；多播就是一点对多点。对于绝大多数应用来说，组间广播，组内单播是最常见的情形
一般比较通用的设计是支持组间广播，不同的组注册不同的订阅。组内的不同机器，如果注册一个相同的ID，则单播；如果注册不同的ID(如IP地址+端口)，则广播。 至于广播关系的维护，一般由于消息队列本身都是集群，所以都维护在公共存储上，如config server、zookeeper等。维护广播关系所要做的事情基本是一致的：1. 发送关系的维护。2. 发送关系变更时的通知。

## 1.2 队列高级特性设计

### 1.2.1 可靠投递（最终一致性）
想要做到完全不丢消息，完全是可能的，前提是消息可能会重复，并且，在异常情况下，要接受消息的延迟。 方案说简单也简单，就是每当要发生不可靠的事情（RPC等）之前，先将消息落地，然后发送。当失败或者不知道成功失败（比如超时）时，消息状态是待发送，定时任务不停轮询所有待发送消息，最终一定可以送达。
对于各种不确定（超时、down机、消息没有送达、送达后数据没落地、数据落地了回复没收到），其实对于发送方来说，都是一件事情，就是消息没有送达。 重推消息所面临的问题就是消息重复。重复和丢失就像两个噩梦，你必须要面对一个。
#### 1.2.1.1 消费确认
当broker把消息投递给消费者后，消费者可以立即响应我收到了这个消息。但收到了这个消息只是第一步，我能不能处理这个消息却不一定。或许因为消费能力的问题，系统的负荷已经不能处理这个消息；或者是刚才状态机里面提到的消息不是我想要接收的消息，主动要求重发。 把消息的送达和消息的处理分开，这样才真正的实现了消息队列的本质-解耦。所以，允许消费者主动进行消费确认是必要的。
#### 1.2.1.2 重复消息和顺序消息
一个主流消息队列的设计范式里，应该是不丢消息的前提下，尽量减少重复消息，不保证消息的投递顺序。
谈到重复消息，主要是两个话题：

1. 如何鉴别消息重复，并幂等的处理重复消息。
2. 一个消息队列如何尽量减少重复消息的投递。
鉴别消息重复需要每一个消息应有一个唯一身份，不论是也无妨自定义还是系统生成的MessageID
处理方式有两个，一是版本号，二是状态机；
#### 1.2.1.3 中间件对于重复消息的处理
1. broker记录MessageId，直到投递成功后清除，重复的ID到来不做处理，这样只要发送者在清除周期内能够感知到消息投递成功，就基本不会在server端产生重复消息。
2. 对于server投递到consumer的消息，由于不确定对端是在处理过程中还是消息发送丢失的情况下，有必要记录下投递的IP地址。决定重发之前询问这个IP，消息处理成功了吗？如果询问无果，再重发。
### 1.2.2 事务
解决方案从大方向上有两种：
1. 两阶段提交，分布式事务。
2. 本地事务，本地落地，补偿发送。
### 1.2.3 push还是pull
#### 1.2.3.1 慢消费
慢消费无疑是push模型最大的致命伤
如果消费者的速度比发送者的速度慢很多，势必造成消息在broker的堆积。假设这些消息都是有用的无法丢弃的，消息就要一直在broker端保存。当然这还不是最致命的，最致命的是broker给consumer推送一堆consumer无法处理的消息，consumer不是reject就是error，然后来回踢皮球。
反观pull模式，consumer可以按需消费，不用担心自己处理不了的消息来骚扰自己，而broker堆积消息也会相对简单，无需记录每一个要发送消息的状态，只需要维护所有消息的队列和偏移量就可以了。
#### 1.2.3.2 消息延迟与忙等
这是pull模式最大的短板。
由于主动权在消费方，消费方无法准确地决定何时去拉取最新的消息。如果一次pull取到消息了还可以继续去pull，如果没有pull取到则需要等待一段时间重新pull。 但等待多久就很难判定了。
#### 1.2.3.3 顺序消息
如果push模式的消息队列，支持分区，单分区只支持一个消费者消费，并且消费者只有确认一个消息消费后才能push送另外一个消息，还要发送者保证全局顺序唯一，听起来也能做顺序消息，但成本太高了，尤其是必须每个消息消费确认后才能发下一条消息，这对于本身堆积能力和慢消费就是瓶颈的push模式的消息队列，简直是一场灾难。
pull模式，如果想做到全局顺序消息，就相对容易很多：

1. producer对应partition，并且单线程。
2. consumer对应partition，消费确认（或批量确认），继续消费即可。
# 2 Kafka的实现
[看完这篇Kafka，你也许就会了Kafka](https://cloud.tencent.com/developer/article/2081270)
[深入理解kafka的架构与工作原理](https://blog.csdn.net/yu1415487509/article/details/136742531)

![[消息队列-kafka实现-架构图.png.png]]
## 2.1 基本概念
kafka名词解释和工作方式：

| 名词                  | 解释                                                                                                               |
| ------------------- | ---------------------------------------------------------------------------------------------------------------- |
| Producer            | 消息生产者，就是向kafka broker发消息的客户端。                                                                                    |
| Consumer            | 消息消费者，向kafka broker取消息的客户端                                                                                       |
| Consumer Group （CG） | 个消费者组由一个或多个消费者实例组成，它们共同消费同一个Topic的分区。                                                                            |
| Broker              | 消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。                                                             |
| Topic               | 一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。                                               |
| Partition           | topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。                                                          |
| Segment             | partition物理上由多个segment组成                                                                                         |
| offset              | 每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.        |
| Replica             | 副本Replication，为保证集群中某个节点发生故障，节点上的Partition数据不丢失，Kafka可以正常的工作，Kafka提供了副本机制，一个Topic的每个分区有若干个副本，一个Leader和多个Follower |
| Leader              | 每个分区多个副本的主角色，生产者发送数据的对象，以及消费者消费数据的对象都是Leader。                                                                    |
| Follower            | 每个分区多个副本的从角色，实时的从Leader中同步数据，保持和Leader数据的同步，Leader发生故障的时候，某个Follower会成为新的Leader。                                 |
## 2.2 存储结构
![[消息队列-kafka-存储结构.png]]
kafka的存储结构基本如上图
### 2.2.1 逻辑层
#### 2.2.1.1 Topic+Partition 的两层结构
kafka 对消息进行了两个层级的分类，分别是 topic 主题和 partition 分区。

将一个主题划分成多个分区的好处是显而易见的。多个分区可以为 kafka 提供可伸缩性、水平扩展的能力，同时对分区进行冗余还可以提高数据可靠性。

不同的分区还可以部署在不同的 broker 上，加上冗余副本就提高了可靠性。
#### 2.2.1.2 Offset
对于追加日志格式，新来的数据只需要往文件末尾追加即可。
对于有多个分区的主题来说，每一个消息都有对应需要追加到的分区（分区器），这个消息在所在的分区中都有一个唯一标识，就是 offset 偏移量
这样的结构具有如下的特点：
- 分区提高了写性能，和数据可靠性；  
- 消息在分区内保证顺序性，但跨分区不保证。  
![[消息队列-kafka-存储结构-offset.png]]
### 2.2.2 物理层
#### 2.2.2.1 日志文件
kafka 使用日志追加的方式来存储数据，新来的数据只要往日志文件的末尾追加即可，这样的方式提高了写的性能。segment文件生命周期由服务端配置参数决定，默认为1GB，存活时长为7天
#### 2.2.2.2 日志索引
kafka 维护了两种索引：偏移量索引和时间戳索引
##### 2.2.2.2.1 偏移量索引
kafka 维护的是一个稀疏索引（sparse index），即不是所有的消息都有一个对应的位置，对于没有位置映射的消息来说，一个二分查找就可以解决了。
##### 2.2.2.2.2 时间戳索引
时间戳索引是一个二级索引，现根据时间戳找到偏移量，然后就可以使用偏移量索引找到对应的消息位置了。
### 2.2.3 零拷贝
为了进一步提升性能，kafka 使用了sendfile零拷贝的技术。

零拷贝简单来说就是在内核态直接将文件内容复制到网卡设备上，减少了内核态与用户态之间的切换。
[零拷贝技术](https://blog.csdn.net/m0_62645012/article/details/139268955)

# 3 可靠性设计
![[消息队列-kafka-可靠性设计.png]]
kafka 通过多副本的方式实现水平扩展，提高容灾性以及可靠性等。这里看看 kafka 的多副本机制。
## 3.1 基本概念

| 简写  | 全文                   | 解释                                                                                                              |
| --- | -------------------- | --------------------------------------------------------------------------------------------------------------- |
| AR  | Assigned Replicas    | 所有的副本统称为 AR。 AR=ISR+OSR                                                                                         |
| ISR | In-Sync Replicas     | ISR 是 AR 的一个子集，即所有和主副本保持同步的副本集合                                                                                 |
| OSR | Out-of-Sync Replicas | OSR 也是 AR 的一个子集，所有和主副本未保持一致的副本集合。kafka 通过一些算法来判定从副本是否保持同步，处于失效的副本也可以通过追上主副本来重新进入 ISR                            |
| LEO | Log End Offset       | LEO 是下一个消息将要写入的 offset 偏移，在 LEO 之前的消息都已经写入日志了，每一个副本都有一个自己的 LEO。                                                 |
| HW  | High Watermark       | 所有和主副本保持同步的副本中，最小的那个 LEO 就是 HW，这个 offset 意味着在这之前的消息都已经被所有的 ISR 写入日志了，消费者可以拉取了，这时即使主副本失效其中一个 ISR 副本成为主副本消息也不会丢失。 |
## 3.2 主副本 HW 与 LEO 的更新
![[消息队列-kafka-HW示意图.png]]
LEO 和 HW 都是消息的偏移量，其中 HW 是所有 ISR 中最小的那个 LEO。
同步过程如下：
1. 生产者将消息发送给 leader；
2. leader 追加消息到日志中，并更新自己的偏移量信息，同时 leader 也维护着 follower 的信息（比如 LEO 等）；
3. follower 向 leader 请求同步，同时携带自己的 LEO 等信息；
4. leader 读取日志，拉取保存的每个 follower 的信息（LEO）；
5. leader 将数据返回给 follower，同时还有自己的 HW；
6. follower 拿到数据之后追加到自己的日志中，同时根据返回的 HW 更新自己的 HW，方法就是取自己的 LEO 和 HW 的最小值
从上面这个过程可以看出，一次同步过程之后 leader 的 HW 并没有增长，只有再经历一次同步，follower 携带上一次更新的 LEO 给 leader 之后，leader 才能更新 HW，这个时候才能能确认消息确实是被所有的 ISR 副本写入成功了。
## 3.3 Leader Epoch
kafka 引入了 leader epoch 的概念，其实这就是一个版本号，在 follower 同步请求中不仅仅传递自己的 LEO，还会带上当前的 LE，当 leader 变更一次，这个值就会增 1。
由于有了 LE 的信息，follower 在崩溃重启之后就不会轻易截断日志，而是会请求最新的信息，避免了上述情况下数据丢失的问题。
# 4 写入数据流程
![[消息队列-kafka-写入流程.png]]
在生产端主要有两个线程：main 和 sender，两者通过共享内存 RecordAccumulator 通信。
### 4.1.1 写入步骤
具体步骤如下：
1. 生产者拦截器在消息发送之前做一些准备工作，比如过滤不符合要求的消息、修改消息的内容等；
2. 序列化器将消息转换成字节数组的形式；
3. 分区器计算该消息的目标分区，然后数据会存储在 RecordAccumulator 中；
4. 发送线程获取数据进行发送；
5. 创建具体的请求；
6. 如果请求过多，会将部分请求缓存起来；
7. 将准备好的请求进行发送；
8. 发送到 kafka 集群；
9. 接收响应；
10. 清理数据。
消息累加器RecordAccumulator会缓存发送数据，缓存大小由参数buffer.memory 配置，默认 32MB；在消息发送时会通过ProducerBatch的形式批量发送请求，从而提高吞吐量（通过 batch.size 控制大小，默认 1MB）
### 4.1.2 发送方式
Producer发送消息默认是异步方式的，如果需要使用同步发送消息，则需要再`Send`之后获取到具体的`Future`的值，通过调用`Future.get()`方法可以暂时阻塞，以达到同步发送的目的。
### 4.1.3 可靠性配置
Kafka为用户提供了三种可靠性级别，用户根据可靠性和延迟的要求进行权衡选择不同的配置。
**ack参数配置**

- `0`：producer不等待broker的ack，这一操作提供了最低的延迟，broker接收到还没有写入磁盘就已经返回，**当broker故障时有可能丢失数据**
- `1`：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将丢失数据。（**只是leader落盘**）
- -1/all  需要全部处于同步状态的副本(ISR)确认写入成功，可靠性最强，性能也差。
### 4.1.4 ExactlyOnce
将服务器的ACK级别设置为-1（all），可以保证producer到Server之间不会丢失数据，即At Least Once。将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once至多一次。

At Least Once可以保证数据不丢失，但是不能保证数据不重复，而At Most Once可以保证数据不重复，但是不能保证数据不丢失，对于重要的数据，则要求数据不重复也不丢失，即Exactly Once即精确的一次。

启用幂等性，即在Producer的参数中设置`enable.idempotence=true`即可，Kafka的幂等性实现实际是将之前的去重操作放在了数据上游来做，开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一个Partition的消息会附带Sequence Number，而Broker端会对`<PID,Partition,SeqNumber>`做缓存，当具有相同主键的消息的时候，Broker只会持久化一条。

但PID在重启之后会发生变化，同时不同的Partition也具有不同的主键，所以幂等性无法保证跨分区跨会话的Exactly Once。
# 5 读取数据
## 5.1 消费模式
 kafka 中的消费是基于拉模式的。消费者通过不断地调用 poll 来获取消息进行消费
### 5.1.1 位移提交
kafka 中的消息都有一个 offset 唯一标识，对于消费者来说，每消费完一个消息需要通知 kafka，这样下次拉取消息的时候才不会拉到已消费的数据（不考虑重复消费的情况）。这个消费者已消费的消息位置就是消费位移
消费者一次可能拉取到多条消息，那么就会有一个提交的方式问题。kafka 默认使用的是自动提交，即五秒自动将拉到的每个分区中最大的消息位移（相关参数是 enable.auto.commit 和 auto.commit.interval.ms）。不过这可能导致重复消费以及数据丢失的问题
### 5.1.2 重复消费
消费者批量pull消息时，如果还没全部消费成功时，消费者挂了。重启后或者再平衡后，会出现重复消费
### 5.1.3 消费丢失
当消费者先提交ack，再消费处理时，消费者挂了，就会出现消费丢失
## 5.2 分区分配策略
一个consumer group中有多个consumer，一个topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由那个consumer消费的问题。
kafka目前有以下3种分配方式
- RoundRobin
- Range
- Sticky
### 5.2.1 RoundRobin
首先，RoundRobin策略，顾名思义，就是轮询分配。它将所有主题的分区按照顺序分配给消费者，实现负载均衡。
RoundRobin策略在消费者数量和分区数量相同的情况下表现最佳，但在实际应用中，我们往往无法保证这一点。

### 5.2.2 Range
Range策略是基于主题的分区数和消费者数量进行分配。它将每个主题的分区数除以消费者数量，得到每个消费者应分配的分区数。
Range策略在消费者数量远大于分区数量时，会导致部分消费者空闲。
### 5.2.3 Sticky
Sticky策略在分配分区时，尽量保证分区与消费者的粘性，即尽量保持原有的分配关系。
在Sticky策略中，假设我们有三个消费者C0、C1和C2，以及两个主题t0和t1，每个主题有三个分区。分配结果如下：
```
C0: t0p0, t1p0
C1: t0p1, t1p1
C2: t0p2, t1p2
```
当消费者C2离开时，分配结果调整为：
```
C0: t0p0, t0p2, t1p0
C1: t0p1, t1p1, t1p2
```
## 5.3 再均衡
消费者之间的协调是通过消费者协调器（ConsumerCoordinator）和组协调器（GroupCoordinator）来完成的。其中一项就是消费者的再均衡。
### 5.3.1 再均衡触发情况
- 有新的消费者加入；  
- 有消费者下线；  
- 有消费者主动退出；  
- 消费组对应的组协调器节点发生变化；  
- 订阅的主题或分区发生数量变化
### 5.3.2 再均衡步骤
- FindCoordinator：消费者查找组协调器所在的机器，然后建立连接；  
- JoinGroup：消费者向组协调器发起加入组的请求；  
- SyncGroup：组协调器将分区分配方案同步给所有的消费者；  
- Heartbeat：消费者进入正常状态，开始心跳。

# 6 KRaft
## 6.1 与zookpper对比
![[消息队列-kafka-Kraft与zookeeper对比.png]]

Kafka有两种不同启动方式：一种是使用Kafka自带的Zookeeper进行启动，另一种是采用Kafka内置的Zookeeper和KRaft协议来启动Kafka。

### 6.1.1 zookeeper启动
#### 6.1.1.1 优势
1. **成熟的解决方案**：Zookeeper 是一个经过广泛测试和验证的分布式协调服务，具有高度的可靠性和稳定性。
2. **易于管理和扩展**：通过 Zookeeper，Kafka 集群的管理和扩展变得更加简单。管理员可以轻松地添加或删除节点，调整集群配置。
3. **强大的故障恢复能力**：Zookeeper 能够快速检测和恢复节点故障，确保 Kafka 集群的高可用性。
#### 6.1.1.2 劣势
1. **额外的依赖**：使用 Zookeeper 模式启动 Kafka 需要额外安装和配置 Zookeeper 服务，增加了系统的复杂性和维护成本。
2. **性能开销**：Zookeeper 在处理大量请求时可能会成为性能瓶颈，尤其是在高并发场景下。
3. **学习曲线**：对于初学者来说，理解和配置 Zookeeper 可能需要一定的时间和精力。
### 6.1.2 kraft启动
#### 6.1.2.1 优势
1. **减少依赖**：KRaft 模式下，Kafka 不再依赖于外部的 Zookeeper 服务，减少了系统的复杂性和维护成本。
2. **性能提升**：通过使用 Raft 共识算法，Kafka 能够更高效地管理元数据，提高系统的整体性能，特别是在高并发场景下。
3. **简化部署**：KRaft 模式使得 Kafka 的部署和维护更加简单，用户无需额外安装和配置 Zookeeper 服务，降低了入门门槛。
#### 6.1.2.2 劣势
1. **兼容性问题**：KRaft 模式相对较新，可能在某些旧版本的 Kafka 中不支持，用户需要确保使用的是最新版本的 Kafka。
2. **学习曲线**：尽管 KRaft 模式简化了部署，但用户仍需了解 Raft 共识算法的基本原理，这可能需要一定的学习时间和精力。
3. **社区支持**：由于 KRaft 模式较新，社区的支持和文档相对较少，用户在遇到问题时可能需要更多的自我探索和研究。
## 6.2 Kraft介绍
KRaft 是 Kafka 内部基于Raft实现的一致性协议，它允许 Kafka 集群在不依赖 ZooKeeper 的情况下运行，从而简化了 Kafka 的架构。
在Kraft模式下，kafka集群的每个节点都允许配置为Controller，Broker角色，也可以都配上，选主模式与ES大致相同
### 6.2.1 Controller[​](https://www.automq.com/blog/how-kafka-achieves-ultimate-cluster-consistency-coordination-based-on-kraft#controller "Direct Link to Controller")

Controller 在生产环境中通常由 3 个节点组成 Quorum，底层使用 KRaft 来进行一致性协调，KRaft 的 Leader 即是 Controller Leader。

只有 Leader 会进行请求处理，Follower 只会跟随 Replay KRaft 中的数据，请求处理流程简要如下：

1. 当 Leader 网络层接收到 Broker 发来的请求后，会将请求首先放入到事件队列中，由后台的单线程来处理事件队列中的请求。通过单线程处理机制简化了并发编程的复杂度，并且确保所有请求可以顺序处理；
    
2. 单线程处理器运行请求对应的 Manager 逻辑。Manager 根据当前内存中维护的状态，生成响应和变更的 Records；
    
3. 最后再把变更的 Records 提交到 KRaft 中，等多数派确认后就可以将响应返回，并 replay(Records) 修改 Manager 维护的内存状态；
    
4. 同时 Follower 也会将 KRaft 中的 Records replay到内存中，内存数据持续的保持同步；
