# RAG流程

![线程状态](https://luxiangdong.com/images/ragone/2.png)
使用信息抽取模型提取用户问题关键信息，bert/gpt
selective_context 
压缩promt
https://luxiangdong.com/2024/01/24/cut80off/

# bert与gpt对比
## 概念
- Bert 基于transformer的编码器部分进行堆叠构建，通过预训练和微调两个阶段来生成深度的双向语言表征，
- gpt基于transformer的解码器部分，通过回归语言模型预训练来学习生成连贯文本的能力
## 上下文理解能力

- bert：由于采用了双向语言模型，bert能够同时考虑前后文信息，因此在理解整个句子或段落时表现出色。适用于需要整个文本的任务，如分类，命名实体识别和句子关系判断
- gpt：作为单向模型，gpt在生成文本时只能依赖已生成的上文，因此在处理需要理解整个文本任务时可能表现不足。适用于生成式的NLP任务
 
## transformer
 Transformer是一种基于自注意力机制（self-attention mechanism）的模型，最初被提出用于处理序列到序列（sequence-to-sequence）的任务，例如机器翻译。它的核心思想是通过自注意力机制来直接捕捉输入序列中的各个位置之间的关系

### 组件：
- 编码器（Encoder）：编码器由多个相同的层堆叠而成，每一层都包括自注意力机制和前馈神经网络（Feedforward Neural Network）。自注意力机制用于捕捉输入序列中各个位置之间的关系，而前馈神经网络则用于对每个位置的特征进行非线性变换。

- 解码器（Decoder）：解码器也由多个相同的层堆叠而成，每一层也包括自注意力机制、编码器-解码器注意力机制和前馈神经网络。解码器的自注意力机制用于捕捉输出序列中各个位置之间的关系，编码器-解码器注意力机制用于将编码器的输出与解码器的输入进行关联。

- 位置编码（Positional Encoding）：由于Transformer模型不具有显式的顺序信息，因此需要添加位置编码来将位置信息嵌入到输入序列中。位置编码通常是一个固定的矩阵，其中的每个位置都对应着一个不同的向量，这些向量会与输入序列的词嵌入相加，以表示每个词在序列中的位置。

- 残差连接（Residual Connection）和层归一化（Layer Normalization）：在Transformer的每个子层中都使用了残差连接和层归一化，以帮助模型更容易地训练和优化。
Transformer模型通过自注意力机制和前馈神经网络来处理输入序列，并通过堆叠多个层来逐步提取输入序列中的特征。这使得Transformer在各种NLP任务中表现出色，如机器翻译、文本生成、语言建模等。